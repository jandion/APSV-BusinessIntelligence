{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "Machine Learning is the study and application of algorithms that generate statistic models that are able to perform a specific task without being specifically coded. Models are created by observing a set of samples and extracting and learning the patterns in the samples data.\n",
    "\n",
    "The most used library in Python to work with ML is SciKit-Learn https://scikit-learn.org/stable/. This module includes the implementations of dozens of different ML algorithms and the functions needed to handle and transform the data that will be used by the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any problem importing those modules, you might need to install or update them. If you are using Anaconda, run an Anaconda Prompt and execute `conda install [package]`, or, if you are using just Python, open a terminal and execute `pip install [package]`.\n",
    "\n",
    "Let's load our well known dataset of train events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/trains.zip\").dropna()\n",
    "data.sheduled_date = data.sheduled_date.astype(\"datetime64[ns]\")\n",
    "data.actual_date = data.actual_date.astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to transform this train event data set into a table in which each row contains the information for a single train trip. These rows will indicate the number of packages that were transported on the train, the number of wagons that were used, the delay in loading and unloading packages, the customer, the train used and the destination station.\n",
    "\n",
    "The code used below includes some advanced functions, such as the `pivot` method. It is not necessary to understand exactly what is happening, but it is advisable to try to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only relevant events\n",
    "trains = data[data.event_type.isin([\"LLEGADA A DESTINO DE VAGONES\", \"SALIDA DE VAGONES\"])]\n",
    "\n",
    "#Calculate delays of each event (in seconds)\n",
    "trains.loc[:,\"delay\"] = (trains.actual_date - trains.sheduled_date).dt.total_seconds()\n",
    "trains = trains.drop(columns=[\"Unnamed: 0\", \"container_type\", \"origin\", \"destination\", \"estimated_date\", \"actual_date\"])\n",
    "\n",
    "\n",
    "#Pivot the table to create columns with the delay of each event type\n",
    "pivot_event=trains.pivot(columns=[\"event_type\"],values=[\"delay\"])\n",
    "pivot_event.columns=pivot_event.columns.map(lambda x: x[0]+\"_\"+x[1].split(\" \")[0])\n",
    "trains = trains.join(pivot_event)\n",
    "\n",
    "#Removing unwanted values\n",
    "trains.loc[trains.event_type==\"SALIDA DE VAGONES\", \"event_location\"] = np.NaN\n",
    "trains.loc[trains.event_type==\"LLEGADA A DESTINO DE VAGONES\", \"sheduled_date\"] = np.NaN\n",
    "#Mixing duplicated rows keeping not null values\n",
    "trains = trains.drop(columns = [\"event_type\", \"delay\"]).groupby([\"item_id\",\"container_id\",\"client\",\"train_id\"],as_index=False).first()\n",
    "\n",
    "#Grouping by train trip, counting the number of packages and waggons\n",
    "trains = trains.groupby(\"sheduled_date\",as_index=False).agg({\"item_id\":\"nunique\", \"container_id\":\"nunique\", \"client\":\"first\", \"train_id\":\"first\", \"event_location\":\"first\", \"delay_LLEGADA\":\"mean\", \"delay_SALIDA\":\"mean\"})\n",
    "\n",
    "#Renaming the final columns\n",
    "trains = trains.drop(columns=[\"sheduled_date\"]).rename(columns={\"item_id\":\"packages\", \"container_id\":\"waggons\"}).dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ML algorithms can only work with numerical values, therefore, we have to convert the columns that contain text (categorical variables) into numbers. The simplest way to do this is by using a LabelEncoder, which just assigns each category to which it finds an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "client_encoder = preprocessing.LabelEncoder().fit(trains.client.unique())\n",
    "train_encoder = preprocessing.LabelEncoder().fit(trains.train_id.unique())\n",
    "location_encoder = preprocessing.LabelEncoder().fit(trains.event_location.unique())\n",
    "\n",
    "for i in range(len(client_encoder.classes_)):\n",
    "    print(f\"\"\"{client_encoder.classes_[i]} encoded as {i}\"\"\")\n",
    "for i in range(len(train_encoder.classes_)):\n",
    "    print(f\"\"\"{train_encoder.classes_[i]} encoded as {i}\"\"\")\n",
    "for i in range(len(location_encoder.classes_)):\n",
    "    print(f\"\"\"{location_encoder.classes_[i]} encoded as {i}\"\"\")\n",
    "\n",
    "trains.client = pd.Series(client_encoder.transform(trains.client.values), index = trains.index)\n",
    "trains.train_id = pd.Series(train_encoder.transform(trains.train_id.values), index = trains.index)\n",
    "trains.event_location = pd.Series(location_encoder.transform(trains.event_location.values), index = trains.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the calculations perfomed in this notebook could be complex and too heavy for some computers. If your computer takes too long to run the experiments, you can reduce the datasets by executing the following code (uncomment it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this method we take sample of a dataset, the parameter frac indicates the percentage of the original data we keep\n",
    "#trains = trains.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test\n",
    "The main idea behind machine learning is that the algorithm will observe a training set of samples, which we should be sure to be representative of the data with which we are working with which. From this training set or training data, the algorithm will learn the patterns that describe the data and generates a model which can replicate the behavior shown on them. Then, using a *different* set of samples, which we call test set or test data, we will evaluate how accurate is the generated model, basically, by measuring the difference between the model output and the correct answer. It is very important to do not use the same data to train and test the model.\n",
    "\n",
    "Let’s see it with an example.\n",
    "We will use a popular dataset in ML called iris dataset. This dataset contains a series of measures of flowers and the type of the flower (for more info, check https://archive.ics.uci.edu/ml/datasets/iris). As we will see later, the task of the model we are going to create is a classification task. \n",
    "The dataset is made up of observations that contain 4 numeric variables and 1 categorical target. The task of the model is to predict the target of a sample by observing its numeric variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is based on one example from the scikit learn documentation \n",
    "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# In this notebook, we will always use x as the feature vector and y as the target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.3)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(x_train, y_train)\n",
    "y_out = clf.predict(x_test)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=\"Accent\")\n",
    "ax.scatter(x_test[:, 0], x_test[:, 1], c=y_out, cmap=\"Accent\", marker = \"x\")\n",
    "ax.scatter(x_test[:, 0], x_test[:, 1], c=y_test, cmap=\"Accent\", alpha=0.3)\n",
    "ax.scatter(x_test[y_out != y_test, 0], x_test[y_out != y_test, 1], facecolors='none', edgecolors='r', s=200)\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = %i)\" % (n_neighbors))\n",
    "plt.show()\n",
    "\n",
    "print(\"Model accuracy: {:.2f}%\".format(np.average(y_out==y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset has 4 dimensions, the plot only shows 2 dimensions, which is easier for humans. The colors in the plot are the different types of flowers. Dots are the training data and cross the test data, the background of each test sample is the correct type, and the sample is surrounded with a red circle if it was misclasified. The accuracy is just the average of the correct classifications made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "Ensure that the partition of the data used to train the model is representative may not be an easy task. For example, in the previous dataset it could be that the data we select to train don't contain samples of one of the class, or may be that the test data were particularly easy to classify.\n",
    "\n",
    "Cross Validation is a group of techniques that solve this problem, all of them are based on the idea of performing several trainings and testings using different partitions each time. The most basic, and most popular, is the K-Fold Cross Validation.\n",
    "In K-Fold, we choose a number k (usually from 5 to 10), then we divide our dataset into k folds. We will train the model k times, in each iteration we will use k-1 folds as the train data and the remaining fold as test data. The evaluation of the final model is calculated as the average of the evaluations for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k = 3\n",
    "kf = KFold(n_splits=k,shuffle=True)\n",
    "accuracies = []\n",
    "i=0\n",
    "for train, test in kf.split(X): # train and test are the indices of the samples that will be used in each set\n",
    "    i+=1\n",
    "    x_train, x_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_out = clf.predict(x_test)\n",
    "\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(9, 8))\n",
    "    ax.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=\"Accent\")\n",
    "    ax.scatter(x_test[:, 0], x_test[:, 1], c=y_out, cmap=\"Accent\", marker = \"x\")\n",
    "    ax.scatter(x_test[:, 0], x_test[:, 1], c=y_test, cmap=\"Accent\", alpha=0.3)\n",
    "    ax.scatter(x_test[y_out != y_test, 0], x_test[y_out != y_test, 1], facecolors='none', edgecolors='r', s=200)\n",
    "    ax.set_title(\"3-Class classification (k = %i) iteration %i\" % (n_neighbors,i))\n",
    "    plt.show()\n",
    "    accuracy = np.average(y_out==y_test)*100\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Iteration {}: Model accuracy: {:.2f}%\".format(i,accuracy))\n",
    "    print(y_test)\n",
    "    print(y_out)\n",
    "print(\"Global accuracy: {:.2f}%\".format(np.average(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Regression is one of the possible tasks for an ML model. The objective is to predict a numeric output based on the input. One example of this could be to predict the number of products that will be sold based on the previous sales or calculate the probability that the stock market will grow tomorrow.\n",
    "\n",
    "\n",
    "The most basic example of this task is to obtain the relation between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import math\n",
    "\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=0)\n",
    "# try to modify the relation between x and y or play with the noise value\n",
    "y = y**2\n",
    "#y = np.sin(y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in the real world are more complicated, the target (variable we want to predict) usually depends on more variables (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "# this makes that plots show in an interactive window\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "x, y = datasets.make_regression(n_samples=100, n_features=2, noise= 0)\n",
    "ax.scatter(x[:,0], x[:,1],y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# reset the plot behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional extra\n",
    "Try to plot the iris dataset in a 3d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some popular models\n",
    "There are many different algorithms or models that can be used for regression tasks. Each of them has its advantages or disadvantages for each kind of problem, but to discuss them is far beyond the scope of this course. We will just try them with their basic configuration and keep the one that best fits our case.\n",
    "\n",
    "#### Linear regression\n",
    "This is the most basic regression model. The algorithm will just try to minimize the error in a function like $Y=aX+b$, where $Y$ is the target and $X$ the vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=10)\n",
    "\n",
    "#y=y**2\n",
    "model = LinearRegression().fit(x,y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x,model.predict(x), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "This model is based on a tree structure in which each node represents a decision and each leaf an output. https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "x_test= np.linspace(x.min(),x.max(), 100)[:, np.newaxis]\n",
    "\n",
    "model = DecisionTreeRegressor().fit(x,y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_test ,model.predict(x_test),\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "This model belongs to a category called ensemble models, because it is made of the aggregation of several models. In the case of random forest, we will have a set of decision trees which will calculate an output, then the output of the random forest will be the average of those outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors\n",
    "Given an input, the output of this algorithm will be the average of the $k$ nearest points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=5).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "This is the most basic Artificial Neural Network https://en.wikipedia.org/wiki/Artificial_neural_network. Opposite  to the other models shown, this one is a \"black box\" and we cannot see how the algorithm calculates the output.\n",
    "\n",
    "Don't worry if you get a warning when running this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = MLPRegressor(max_iter=50000).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate a regression model\n",
    "We have trained 5 models, and we can guess that some are better than others, but we need a formal way to evaluate the models. There are several ways to evaluate a regression model, all of them based on the __error between the output of the model and the real answer__ when evaluating the test data, so the lower the error, the better the model. Let see some of them:\n",
    "\n",
    "1. Max error: $max(| y_i - \\hat{y}_i |)$\n",
    "\n",
    "2. Mean absolute error or MAE: $\\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$\n",
    "\n",
    "3. Mean squared error or MSE: $\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Don't worry about the math, scikit learn will do everything for us!\n",
    "\n",
    "Remember about K-Folds Cross Validation when evaluating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_absolute_error, mean_squared_error, max_error\n",
    "models = {\"Linear Regression\":LinearRegression(), \\\n",
    "          \"Decision Tree\": DecisionTreeRegressor(), \\\n",
    "          \"Random Forest\": RandomForestRegressor(n_estimators=100), \\\n",
    "          \"Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5), \\\n",
    "          \"Perceptron\": MLPRegressor(max_iter=2000)}\n",
    "\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=10)\n",
    "k = 3\n",
    "kf = KFold(n_splits=k,shuffle=True)\n",
    "for name in models:\n",
    "    print(\"Start training models of {}\".format(name))\n",
    "    i = 0\n",
    "    me_global = []\n",
    "    mae_global = []\n",
    "    mse_global = []\n",
    "    for train, test in kf.split(x):\n",
    "        i+=1\n",
    "        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "        model = models[name].fit(x_train,y_train)\n",
    "        y_out = model.predict(x_test)\n",
    "        # We have to provide the real value of the target and the model's output\n",
    "        me = max_error(y_test, y_out)\n",
    "        mae = mean_absolute_error(y_test, y_out)\n",
    "        mse = mean_squared_error(y_test, y_out)\n",
    "        me_global.append(me)\n",
    "        mae_global.append(mae)\n",
    "        mse_global.append(mse)\n",
    "        print(\"Iteration {}: me={:.4f} mae={:.4f} mse={:.4f}\".format(i,me, mae, mse))\n",
    "    print(\"Global: me={:.4f} mae={:.4f} mse={:.4f}\\n\".format(np.average(me_global), np.average(mae_global), np.average(mse_global)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it\n",
    "Now is your turn to create a \"real world\" model. Just after reading the datasets, we have transformed and prepared them in a format suitable for the algorithms we have just seen.\n",
    "\n",
    "The target is to predict the delay of a train arriving to the destination, that is the `delay_LLEGADA` column. We will use a vector of features that contains all the other columns in the train dataset. \n",
    "\n",
    "The task is to train some regression models using different algorithms and keep the best one. Then answer the following questions:\n",
    "\n",
    "- What is the best algorithm?\n",
    "- What measure have you used to select the best model?\n",
    "- Do you think that the best model is a \"good\" regression model? Why?\n",
    "- Can you think of a variable that we can add to our features to improve the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Another common task in ML is classification; in this case the target will be a class or a categorical variable, like in the iris dataset.\n",
    "\n",
    "When talking about classifications task there are two possibilities: the target can only be positive or negative (binary classification) or the target can be any class of a set of categories (multi class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_classification(n_samples=300, n_features=2,n_classes=4,n_redundant=0,n_clusters_per_class=1)\n",
    "# try to modify the relation between x and y or play with the noise value\n",
    "#y = y**2\n",
    "#y = np.sin(y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "# this makes that plots show in an interactive window\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "x, y = datasets.make_classification(n_samples=300, n_features=3,n_classes=4,n_redundant=0,n_clusters_per_class=1)\n",
    "ax.scatter(x[:,0], x[:,1], x[:,2],c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# reset the plot behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some popular models\n",
    "Like in the case of regression there are many algorithms for classification. Many of them have implementations for both regression and classification. \n",
    "\n",
    "In the following plots, the decision regions will be plotted. Those are the regions that the model will use to perform the classification, an input will be classified depending on which region it appears.\n",
    "\n",
    "#### Logistic regression\n",
    "Don't be misleading with the regression word, this model can only be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x, y = datasets.make_classification(n_samples=300, n_features=2,n_classes=3,n_redundant=0,n_clusters_per_class=1)\n",
    "\n",
    "# For decision regions\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "model = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\").fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier().fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(max_iter=2000).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate a classification model\n",
    "In this case the output is a class or a label, so we can't calculate the error, instead we will calculate a ratio between the number of times that the output is correct and another count of the outputs, the total number or the number of wrong outputs for example. For example, the most intuitive way to evaluate a classification model is the accuracy : How many times my model guess the correct answer over all the anwsers?. But, even if this can be useful in some cases, in practice, it is not an informative measure.\n",
    "\n",
    "### Confusion matrix\n",
    "This is the main tool for evaluating a classification model https://en.wikipedia.org/wiki/Confusion_matrix. It is defined as:\n",
    "\n",
    "\"By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$.\"\n",
    "\n",
    "In other words, it is a matrix where columns represent the true class and rows represent the output class. With it we can know how many samples have been correctly classified and which classes can be confused.\n",
    "\n",
    "\n",
    "In the case of binary classification, the matrix has only 2 columns and 2 rows, and each of the cells has a proper name.\n",
    "\n",
    "There are some metrics obtained from these cells:\n",
    "1. Precision: the ratio between positive predicted and the total of positive samples $\\frac{TP}{TP+FP}$\n",
    "2. Accuracy: the ratio of correctly predicted over the total of samples $\\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "3. Recall: the ratio of correctly predicted over all predicted as positive $\\frac{TP}{TP+FN}$\n",
    "4. F1 score: harmonic mean of precision and recall $\\frac{Precision*Recall}{Precision+Recall}=\\frac{2*TP}{2*TP+FP+FN}$\n",
    "\n",
    "Usually more complex metrics are used, but for this course the F1 score is enough. And again, don't worry, scikit learn includes all these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "models = {\"Logistic Regression\":LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=2000), \\\n",
    "          \"Decision Tree\": DecisionTreeClassifier(), \\\n",
    "          \"Random Forest\": RandomForestClassifier(n_estimators=100), \\\n",
    "          \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), \\\n",
    "          \"Perceptron\": MLPClassifier(max_iter=2000)}\n",
    "\n",
    "x, y = datasets.make_classification(n_samples=300)\n",
    "\n",
    "for name in models:\n",
    "    print(\"Start training models of {}\".format(name))\n",
    "    i = 0\n",
    "    f1_global = []\n",
    "    for train, test in kf.split(x):\n",
    "        i+=1\n",
    "        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "        model = models[name].fit(x_train,y_train)\n",
    "        y_out = model.predict(x_test)\n",
    "        # We have to provide the real value of the target and the model's output\n",
    "        f1_global.append(f1_score(y_test, y_out, average=\"weighted\"))\n",
    "        print(confusion_matrix(y_test, y_out))\n",
    "        print(classification_report(y_test, y_out))\n",
    "    print(\"Global: f1={:.4f}\\n\".format(np.average(f1_global)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it\n",
    "In this case, although we will use the same dataframe, our target is to know which client created the logistics order. As in the case of regression, we will use the rest of the columns as features of the model. Find which model works better with this data and answer the following questions:\n",
    "\n",
    "- What is the best algorithm?\n",
    "- What is the average F1 score for that algorithm?\n",
    "- Which type of client is the most difficult to classify? And the easiest? \n",
    "- Which client is most often confused by CLIENT_0? • Which client is usually misclassified as CLIENT_0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
