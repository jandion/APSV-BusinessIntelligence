{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we will work with a set of data obtained from a real logistics process. This data contains events of a logistics process in which a series of goods or items are transported from one station to another by train. Trains move wagons in which various items can be transported. For each event, 3 instants of time are recorded:\n",
    "1.\tthe time in which the event is planned\n",
    "2.\tthe time the event was estimated\n",
    "3.\tthe time at which the event actually occurred\n",
    "\n",
    "The fields in the dataset have been anonymized, except for the times and description of the events. The types of events appear in Spanish, the following table is a translation of these into English:\n",
    "\n",
    "Spanish | English\n",
    "---|---\n",
    "'EXPEDICION DE VAGONES'          | 'EXPEDITION OF WAGONS'\n",
    "'FIN DE CARGA DE VAGONES'        | 'END OF WAGON LOADING'\n",
    "'FIN DE DESCARGA DE VAGONES'     | 'END OF WAGON UNLOADING'\n",
    "'LLEGADA A DESTINO DE VAGONES'   | 'ARRIVAL AT DESTINATION OF WAGONS'\n",
    "'LLEGADA DE MERCANCIAS'          | 'ARRIVAL OF ITEMS'\n",
    "'LLEGADA DE VAGONES'             | 'ARRIVAL OF WAGONS'\n",
    "'SALIDA DE VAGONES'              | 'DEPARTURE OF WAGONS'\n",
    "\n",
    "\n",
    "Our goal is obtain as much information as we can from this dataset. We will achieve this in two ways: we will answer questions with numeric values (e.g. how many trains take part in the proccess?) or we will generate some charts to present information in a visual way (e.g. how is the distribution of items transported throughout the year?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = pd.read_csv(\"data/trains.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration\n",
    "Any project related with data analysis starts with a study of the data itself: \n",
    "* What kind of data do we have?\n",
    "* How is it organized? How many columns have each dataframe?\n",
    "* Are there wrong or missing values?\n",
    "* What does each value of a column means? How many different values are in each column?\n",
    "\n",
    "With the examples we saw in the previous workshop try to answer that questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.event_location.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "After a first view of the data and before to start working with it, we must clean it. This proccess is called preprocessing and it is crucial to be able to obtain good results. We will discard invalid data, fill missing values, drop redundant information, correct typos, etc. We need to create a dataset with the following restrictions.\n",
    "\n",
    "* All columns must contain relevant information\n",
    "* Rows with null or incorrect values should be discarded unless these values can be retrieved in some way (https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html, https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html and https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)\n",
    "* The types of the columns must correspond to the type of data they contain (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)\n",
    "\n",
    "Python and Pandas have a multitude of methods that make it easier to work with dates. In this workshop can be useful those that allow us to obtain certain fragments of a date (hour of day, the day of the week, etc.) https://docs.python.org/3/library/datetime.html. An effective way to apply a change to an entire column is with a syntax like `data.planned_date.hour`, which allows us to get the hour of the dates in the column `planned_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics\n",
    "\n",
    "Once we know about the data we are using and we have cleaned it, we need to ask what kind of useful information we can extract from that data. It is a good idea to make a kind of brainstorming of possible questions, then take the list of resulting questions and sort them by their difficulty, finally begin to answer them starting by the easiest ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic results\n",
    "\n",
    "Some of the esaiest question or more basic results we can obtain are the following:\n",
    "* Numeric results\n",
    "    * How many packages are there?\n",
    "    * How many trains are there?\n",
    "    * How many wagons are there?\n",
    "    * How many stations are there?\n",
    "    * How many routes are there? (combination of origin and destination)\n",
    "    * Which is the most used train?\n",
    "    * How many types of events are there?\n",
    "    * What is the average delay of each type of event?\n",
    "    * How many packages are processed at each of the stations?\n",
    "\n",
    "* Graphic results\n",
    "    * Bar chart of the number of packages processed at each station\n",
    "    * Histogram of the delay in minutes\n",
    "    * Distributions of the number of packages handled according to dates (all three), i.e. the number of unique item_id per day\n",
    "\n",
    "\n",
    "By answering these questions, you may discover some inconsistencies or errors in the data that you did not detect in the cleaning phase. This is usual in this type of task, you can return to the preprocessing phase with this new knowledge and refine the cleaning of the data.\n",
    "\n",
    "Some of those questions are just countings, sumations or rankings, but other need data aggregations with ``groupby``.\n",
    "\n",
    "**Answer each question in a different cell of this notebook (remember that you can insert new cell from the Insert menu).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate results\n",
    "\n",
    "* Numeric results\n",
    "    * How many different routes does each of the trains do? \n",
    "    * What are the average, max, and min durations of the complete process of a package? (difference between the first and last date registered for an item)\n",
    "    * What is the day of the week on which more shipments are made?\n",
    "    * In which months each train is active?, and on what days of the week?\n",
    "    * How long does it take on average to load a wagon?\n",
    "    * How many packages does each train carry on each journey? And how many wagons?\n",
    "    * In which stations does each client operate?\n",
    "\n",
    "* Graphic results\n",
    "    * Distribution of the delay in minutes of the event x by train\n",
    "    * Number of packages (cumulative) that each station has processed over time\n",
    "    * Distribution of delays according to the type of event\n",
    "\n",
    "**Answer at least 5 of these questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced results\n",
    "\n",
    "* Identification of stations\n",
    "    The sources and destinations that appear in the data correspond to platforms of the different stations. For example, LOCATION 19 is a platform of STATION 8. The task consists of creating two new columns, origin_station and destination_station obtained from identifying which locations correspond to each station. \n",
    "\n",
    "    Tip: This is an exploration task that can be done in several ways, one way to start is with the meaning of events. The event \" LLEGADA A DESTINO DE VAGONES \" is always recorded at the destination station of an item.\n",
    "\n",
    "\n",
    "* Processes: \n",
    "    The life cycle of each package can be viewed as a sequence of states representing the logistics process.\n",
    "    * How many different process sequences appear in the data? Which one or which are correct?\n",
    "    * Among those that are correct, which are the most frequent?\n",
    "    * How could you correct the wrong ones?\n",
    "\n",
    "**Optional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
